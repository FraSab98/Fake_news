{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FraSab98/Fake_news/blob/main/Fake_news_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvHSC6Pp6-CT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from textblob import TextBlob\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU disponibile?\", torch.cuda.is_available())\n",
        "print(\"Dispositivo selezionato:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ],
      "metadata": {
        "id": "Fid5owQMVz71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9z3YuGs7DZT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"./dataset/WELFake_Dataset.csv\")\n",
        "df.head()\n",
        "df = df.dropna()\n",
        "df.reset_index(inplace=True)\n",
        "df = df.drop(['Unnamed: 0', 'title'], axis=1)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    lm = WordNetLemmatizer()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub(\"\\\\W\", \" \", text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    # Rimozione di caratteri speciali e numeri\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Tokenizzazione\n",
        "    tokens = word_tokenize(text)\n",
        "    # Rimozione di stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lm.lemmatize(x) for x in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "df['clean_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "def count_uppercase_by_label(text, label):\n",
        "    if isinstance(text, str):  # Verifica che sia una stringa\n",
        "        # Conta le lettere maiuscole\n",
        "        uppercase_count = sum(1 for char in text if char.isupper())\n",
        "\n",
        "        # # Restituisce il conteggio in base alla label\n",
        "        # if label == 0:\n",
        "        return uppercase_count  # Se label è 0, restituisce il conteggio delle maiuscole per la label 0\n",
        "        # elif label == 1:\n",
        "        #     return uppercase_count  # Se label è 1, restituisce il conteggio delle maiuscole per la label 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "df['uppercase_count'] = df.apply(lambda row: count_uppercase_by_label(row['text'], row['label']), axis=1)\n",
        "\n",
        "# Somma totale delle lettere maiuscole\n",
        "total_uppercase = df['uppercase_count'].sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.countplot(x = df['label'], palette = 'Set1', alpha = 0.8)\n",
        "plt.title('Distribution of Fake - 0 /Real - 1 News')\n",
        "plt.savefig('./grafici/Distribution_of_Fake_Real.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "izZedJ7ZC6Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df['num_words'] = df['text'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize = (14,5))\n",
        "sns.histplot(df['num_words'], bins = range(1, 3000, 50), palette = 'Set1', alpha = 0.8)\n",
        "plt.title('Distribution of the News Words count')\n",
        "plt.savefig('./grafici/Distribution_of_num_words.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "G_lsJhp3Dao0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCdDWU127PYv"
      },
      "outputs": [],
      "source": [
        "# Calcolo della frequenza delle parole\n",
        "# Calcola il TF-IDF delle parole nel DataFrame\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "word_tfidf = vectorizer.fit_transform(df['clean_text']).toarray()\n",
        "\n",
        "# Crea un DataFrame con i valori TF-IDF\n",
        "df_tfidf = pd.DataFrame(word_tfidf, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Somma i valori TF-IDF su tutte le righe (documenti) per ogni parola\n",
        "word_tfidf_sum = df_tfidf.sum(axis=0)\n",
        "\n",
        "# Converte i valori TF-IDF in un dizionario (parola -> TF-IDF)\n",
        "word_tfidf_dict = word_tfidf_sum.to_dict()\n",
        "\n",
        "# Calcolare la lunghezza del testo per ogni riga\n",
        "df['text_length'] = df['text'].apply(lambda x: len(x.replace(\" \", \"\")))\n",
        "\n",
        "# Calcolare la media della lunghezza del testo\n",
        "df['avg_text'] = mean_length = df['text_length'].mean()\n",
        "\n",
        "df['polarity'] = df['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "df['subjectivity'] = df['clean_text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
        "\n",
        "struct_features = df[['polarity', 'subjectivity', 'text_length', 'uppercase_count']].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Salva il vectorizer su disco\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")"
      ],
      "metadata": {
        "id": "P3jUlURDgUZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkZkT6NT7TRa"
      },
      "outputs": [],
      "source": [
        "# Normalizzazione delle feature strutturali\n",
        "scaler = StandardScaler()\n",
        "normalized_structural_features = scaler.fit_transform(struct_features)\n",
        "\n",
        "X_struct = np.hstack([struct_features])\n",
        "\n",
        "# feature strutturali e tf-idf\n",
        "X_tfidf = np.hstack([normalized_structural_features, df_tfidf])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler, \"scaler_struct_features.pkl\")"
      ],
      "metadata": {
        "id": "_FJv5ce8nbHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eUzF3xU7fmm"
      },
      "outputs": [],
      "source": [
        "class Evaluation:\n",
        "\n",
        "    def __init__(self, model, x_train, x_test, y_train, y_test):\n",
        "        self.model = model\n",
        "        self.x_train = x_train\n",
        "        self.x_test = x_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def train_evaluation(self):\n",
        "        y_pred_train = self.model.predict(self.x_train)\n",
        "\n",
        "        acc_scr_train = accuracy_score(self.y_train, y_pred_train)\n",
        "        print(\"Accuracy Score On Training Data Set :\", acc_scr_train)\n",
        "        print()\n",
        "\n",
        "        # con_mat_train = confusion_matrix(self.y_train, y_pred_train)\n",
        "        # print(\"Confusion Matrix On Training Data Set :\\n\", con_mat_train)\n",
        "        # print()\n",
        "\n",
        "        class_rep_train = classification_report(self.y_train, y_pred_train)\n",
        "        print(\"Classification Report On Training Data Set :\\n\", class_rep_train)\n",
        "\n",
        "    def test_evaluation(self):\n",
        "        y_pred_test = self.model.predict(self.x_test)\n",
        "\n",
        "        acc_scr_test = accuracy_score(self.y_test, y_pred_test)\n",
        "        print(\"Accuracy Score On Testing Data Set :\", acc_scr_test)\n",
        "        print()\n",
        "\n",
        "        # con_mat_test = confusion_matrix(self.y_test, y_pred_test)\n",
        "        # print(\"Confusion Matrix On Testing Data Set :\\n\", con_mat_test)\n",
        "        # print()\n",
        "\n",
        "        class_rep_test = classification_report(self.y_test, y_pred_test)\n",
        "        print(\"Classification Report On Testing Data Set :\\n\", class_rep_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINE TUNING BERT FOR CLASSIFICATION\n"
      ],
      "metadata": {
        "id": "E9y0cTxOGyoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "WJ8b-1qRfVHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import DataCollatorWithPadding\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Controlla se la GPU è disponibile\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Inizializza il tokenizer e il modello BERT base cased\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "# bert_model = BertModel.from_pretrained('bert-base-cased').to(device)\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "\n",
        "id2label = {0: \"Fake\", 1: \"Real\"}\n",
        "label2id = {\"Fake\": 0, \"Real\": 1}\n",
        "\n",
        "# Tokenizer e modello\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Dividi il dataset in 70k per training/validation e 5k per test finale\n",
        "train_val_df, test_df = train_test_split(df, test_size=5000, random_state=42, stratify=df['label'])\n",
        "\n",
        "# Verifica\n",
        "print(len(train_val_df))  # 70000\n",
        "print(len(test_df))       # 5000\n",
        "\n",
        "# Dividi ulteriormente i 70k in training e validation\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "# Verifica\n",
        "print(len(train_df))  # 56000\n",
        "print(len(val_df))    # 14000\n",
        "\n",
        "# Converte in Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df[['clean_text', 'label']])\n",
        "val_dataset = Dataset.from_pandas(val_df[['clean_text', 'label']])\n",
        "test_dataset = Dataset.from_pandas(test_df[['clean_text', 'label']])\n",
        "\n",
        "# dataset = Dataset.from_pandas(df[['clean_text', 'label']])\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"clean_text\"], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# dataset = dataset.map(tokenize, batched=True)\n",
        "# Applica il tokenizer a ciascun set\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize, batched=True)\n",
        "# test_dataset = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Carica BERT per classificazione binaria\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",num_labels=2,\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id).to(device)\n",
        "\n",
        "# print layers\n",
        "# for name, param in model.named_parameters():\n",
        "#    print(name, param.requires_grad)\n",
        "\n",
        "# freeze base model parameters\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze base model pooling layers\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    if \"pooler\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# print layers\n",
        "# for name, param in model.named_parameters():\n",
        "#    print(name, param.requires_grad)\n",
        "\n"
      ],
      "metadata": {
        "id": "2d4pNe7Rbnot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "import json\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# load metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "auc_score = evaluate.load(\"roc_auc\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1_score = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics2(eval_pred):\n",
        "    # get predictions\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # apply softmax to get probabilities\n",
        "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
        "    # use probabilities of the positive class for ROC AUC\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "    # compute auc\n",
        "    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3)\n",
        "\n",
        "    # predict most probable class\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    # compute accuracy\n",
        "    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'], 3)\n",
        "    prec = np.round(precision.compute(predictions=predicted_classes, references=labels)['precision'], 3)\n",
        "    rec = np.round(recall.compute(predictions=predicted_classes, references=labels)['recall'], 3)\n",
        "    f1 = np.round(f1_score.compute(predictions=predicted_classes, references=labels)['f1'], 3)\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": acc,\n",
        "        \"AUC\": auc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1\": f1\n",
        "    }\n",
        "\n",
        "# with open(\"./risultati/log_history.json\", \"r\") as f:\n",
        "#     log_history = json.load(f)\n",
        "\n",
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./risultati\",\n",
        "    learning_rate=lr,\n",
        "    fp16=torch.cuda.is_available(),  # automatico e sicuro\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"tensorboard\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics2,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "Xdch3T9ufj0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Assicurati che queste variabili siano definite prima di questo blocco\n",
        "# (es. tokenizer, model, train_dataset, val_dataset, TrainingArguments, Trainer)\n",
        "# ... (il tuo codice precedente fino a trainer.train()) ...\n",
        "\n",
        "# Recupera le metriche loggate\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# --- MODIFICA QUI PER CALCOLARE LE MEDIE PER EPOCA ---\n",
        "\n",
        "# Dizionari per accumulare i valori per ogni epoca\n",
        "# Useremo defaultdict per semplificare l'aggiunta di valori\n",
        "epoch_data = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "for log in log_history:\n",
        "    if \"epoch\" in log:\n",
        "        epoch = round(log[\"epoch\"]) # Arrotonda l'epoca all'intero più vicino\n",
        "\n",
        "        # Log di metriche di valutazione (eval_Accuracy, eval_AUC, ecc.)\n",
        "        # Queste vengono loggate una volta per epoca, quindi le prendiamo direttamente\n",
        "        if \"eval_Accuracy\" in log:\n",
        "            epoch_data[epoch][\"eval_Accuracy\"].append(log[\"eval_Accuracy\"])\n",
        "            epoch_data[epoch][\"eval_AUC\"].append(log[\"eval_AUC\"])\n",
        "            epoch_data[epoch][\"eval_Precision\"].append(log[\"eval_Precision\"])\n",
        "            epoch_data[epoch][\"eval_Recall\"].append(log[\"eval_Recall\"])\n",
        "            epoch_data[epoch][\"eval_F1\"].append(log[\"eval_F1\"])\n",
        "\n",
        "        # Log della loss di training\n",
        "        # La loss viene loggata più volte per epoca (ogni logging_steps)\n",
        "        # Se logging_strategy=\"epoch\", 'loss' dovrebbe già essere la media dell'epoca,\n",
        "        # ma per sicurezza o se vuoi una granularità diversa, potresti voler sommare.\n",
        "        # Per semplicità, se 'loss' è presente, lo consideriamo come il valore di training per quel punto.\n",
        "        # Se vuoi la media su tutti gli step dell'epoca, dovresti accumulare\n",
        "        # e poi fare la media alla fine dell'epoca.\n",
        "        # Dato che logging_strategy=\"epoch\", il log con 'loss' dovrebbe essere già la media dell'epoca.\n",
        "        if \"loss\" in log:\n",
        "            # Assumiamo che 'loss' quando loggato con logging_strategy=\"epoch\" sia la media dell'epoca.\n",
        "            # Se fosse la loss di un singolo step, dovremmo accumularla e fare la media.\n",
        "            # Per il Trainer di HF con logging_strategy=\"epoch\", 'loss' è la media dell'epoca.\n",
        "            epoch_data[epoch][\"train_loss\"].append(log[\"loss\"])\n",
        "\n",
        "\n",
        "# Estrai i valori medi per ogni epoca\n",
        "epochs_list = sorted(epoch_data.keys())\n",
        "eval_accuracy = [np.mean(epoch_data[e][\"eval_Accuracy\"]) if epoch_data[e][\"eval_Accuracy\"] else None for e in epochs_list]\n",
        "eval_auc = [np.mean(epoch_data[e][\"eval_AUC\"]) if epoch_data[e][\"eval_AUC\"] else None for e in epochs_list]\n",
        "eval_precision = [np.mean(epoch_data[e][\"eval_Precision\"]) if epoch_data[e][\"eval_Precision\"] else None for e in epochs_list]\n",
        "eval_recall = [np.mean(epoch_data[e][\"eval_Recall\"]) if epoch_data[e][\"eval_Recall\"] else None for e in epochs_list]\n",
        "eval_f1 = [np.mean(epoch_data[e][\"eval_F1\"]) if epoch_data[e][\"eval_F1\"] else None for e in epochs_list]\n",
        "\n",
        "# Per la train_loss, se logging_strategy=\"epoch\", ci sarà un solo valore 'loss' per epoca.\n",
        "# Se ci fossero più valori (es. per logging_steps), np.mean li media.\n",
        "train_loss = [np.mean(epoch_data[e][\"train_loss\"]) if epoch_data[e][\"train_loss\"] else None for e in epochs_list]\n",
        "\n",
        "# Rimuovi i None se alcune metriche non sono state loggate per tutte le epoche\n",
        "# (utile se l'addestramento si è interrotto o se ci sono log incompleti)\n",
        "# Filtra solo le epoche per cui tutti i dati necessari sono disponibili per evitare None nel plot\n",
        "valid_epochs_indices = [i for i, val in enumerate(eval_accuracy) if val is not None and eval_auc[i] is not None and train_loss[i] is not None]\n",
        "epochs_filtered = [epochs_list[i] for i in valid_epochs_indices]\n",
        "eval_accuracy_filtered = [eval_accuracy[i] for i in valid_epochs_indices]\n",
        "eval_auc_filtered = [eval_auc[i] for i in valid_epochs_indices]\n",
        "eval_precision_filtered = [eval_precision[i] for i in valid_epochs_indices]\n",
        "eval_recall_filtered = [eval_recall[i] for i in valid_epochs_indices]\n",
        "eval_f1_filtered = [eval_f1[i] for i in valid_epochs_indices]\n",
        "train_loss_filtered = [train_loss[i] for i in valid_epochs_indices]\n",
        "\n",
        "\n",
        "# --- PLOTTING ---\n",
        "\n",
        "# Plot 1: Validation Metrics over Time\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs_filtered, eval_accuracy_filtered, label=\"Accuracy\", marker='o')\n",
        "plt.plot(epochs_filtered, eval_auc_filtered, label=\"AUC\", marker='x')\n",
        "plt.plot(epochs_filtered, eval_precision_filtered, label=\"Precision\", marker='^')\n",
        "plt.plot(epochs_filtered, eval_recall_filtered, label=\"Recall\", marker='s')\n",
        "plt.plot(epochs_filtered, eval_f1_filtered, label=\"F1 Score\", marker='d')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Validation Metrics over Time\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./grafici/val_all_metrics.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# Plot 2: Accuracy e AUC\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs_filtered, eval_accuracy_filtered, label=\"Validation Accuracy\", marker='o')\n",
        "plt.plot(epochs_filtered, eval_auc_filtered, label=\"Validation AUC\", marker='x')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Validation Accuracy & AUC over Time\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"./grafici/val_accuracy_auc.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot 3: Loss di Training (media per epoca)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(epochs_filtered, train_loss_filtered, label=\"Training Loss\", color=\"red\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss over Time (Average per Epoch)\") # Titolo più descrittivo\n",
        "plt.grid()\n",
        "plt.savefig(\"./grafici/train_loss.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"Grafici generati e salvati nella cartella ./grafici/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTixqMhxJ2CM",
        "outputId": "86285bff-7df2-47dd-ef3c-89082cd10954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grafici generati e salvati nella cartella ./grafici/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "shutil.make_archive('model', 'zip', 'modelli')\n",
        "shutil.make_archive('graf', 'zip', 'grafici')\n",
        "# shutil.make_archive('ris', 'zip', 'risultati')\n",
        "shutil.make_archive('checkpoint-41600', 'zip', './risultati/checkpoint-41600')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('graf.zip')\n",
        "files.download('model.zip')\n",
        "# files.download('ris.zip')\n",
        "files.download('checkpoint-41600.zip')\n",
        "\n"
      ],
      "metadata": {
        "id": "RhOIw2Cymt4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./modello_finale\")\n",
        "tokenizer.save_pretrained(\"./modello_finale\")\n"
      ],
      "metadata": {
        "id": "ivVY2qGcsWGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# === Carica modello e tokenizer ===\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"./risultati/checkpoint-41600\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"./risultati/checkpoint-41600\")\n",
        "model.eval()\n",
        "\n",
        "# === Dataset personalizzato ===\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# === Prepara dati ===\n",
        "texts = test_dataset['clean_text']    # lista di stringhe del nuovo dataset\n",
        "labels = test_dataset['label']    # lista di etichette vere\n",
        "test_dataset = TestDataset(texts, labels, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# === Valutazione ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels_batch = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())  # classe positiva\n",
        "        all_labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "# === Metriche finali ===\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(\"ROC-AUC:\", roc_auc_score(all_labels, all_probs))\n",
        "\n",
        "# === Stampa etichetta predetta e vera per ogni campione ===\n",
        "for pred, true_label in zip(all_preds, all_labels):\n",
        "    print(f\"Predetto: Fake \" if pred == 0 else f\"Predetto: Real\")\n",
        "    print(f\"Reale: {true_label}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'predicted': all_preds,\n",
        "    'true': all_labels\n",
        "})\n",
        "\n",
        "results_df.to_csv('predizioni_test.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "su5LsY2Vs4Do",
        "outputId": "9aec3fdd-5396-4736-d0ac-538ed8a4e30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Column 'clean_text' doesn't exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-383540889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# === Prepara dati ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# lista di stringhe del nuovo dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# lista di etichette vere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2741604504.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         encoding = self.tokenizer(\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_select_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, column_name)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column '{column_name}' doesn't exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Column 'clean_text' doesn't exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Converti in array numpy per sklearn metrics\n",
        "all_preds = np.array(all_preds)\n",
        "all_probs_positive_class = np.array(all_probs)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# --- 5. Calcolo delle Metriche Finali ---\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"REPORT DI CLASSIFICAZIONE:\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "roc_auc = roc_auc_score(all_labels, all_probs_positive_class) # Usiamo roc_auc_score direttamente qui\n",
        "\n",
        "print(f\"Accuracy: {acc:.3f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "\n",
        "# Calcola le metriche per classe\n",
        "prec_per_class, rec_per_class, f1_per_class, _ = precision_recall_fscore_support(all_labels, all_preds, labels=[0, 1])\n",
        "\n",
        "# Calcola le metriche aggregate (weighted average)\n",
        "prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "\n",
        "# 6.1. Matrice di Confusione\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "labels_cm = [\"Classe 0\", \"Classe 1\"] # O \"Fake\", \"Real\" a seconda delle tue classi\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels_cm, yticklabels=labels_cm, cbar=False)\n",
        "plt.ylabel('Etichetta Vera', fontsize=12)\n",
        "plt.xlabel('Etichetta Predetta', fontsize=12)\n",
        "plt.title('Matrice di Confusione', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"grafici/confusion_matrix.png\")\n",
        "plt.close()\n",
        "\n",
        "# 6.2. Precision, Recall, F1-Score per Classe\n",
        "plt.figure(figsize=(9, 6))\n",
        "bar_width = 0.25\n",
        "x = np.arange(2) # Per Classe 0 e Classe 1\n",
        "\n",
        "plt.bar(x - bar_width, prec_per_class, width=bar_width, label='Precision', color='skyblue')\n",
        "plt.bar(x, rec_per_class, width=bar_width, label='Recall', color='lightcoral')\n",
        "plt.bar(x + bar_width, f1_per_class, width=bar_width, label='F1-Score', color='lightgreen')\n",
        "\n",
        "plt.xticks(x, labels_cm, fontsize=10) # Usa le stesse etichette della matrice di confusione\n",
        "plt.ylabel('Valore Metrica', fontsize=12)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.title('Precision, Recall, F1-Score per Classe', fontsize=14)\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"grafici/metrics_per_class.png\")\n",
        "plt.close()\n",
        "\n",
        "# 6.3. ROC Curve\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs_positive_class) # Usa all_probs_positive_class\n",
        "roc_auc_final = auc(fpr, tpr) # Calcola AUC dalla curva ROC\n",
        "\n",
        "plt.figure(figsize=(8, 7))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc_final:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classificatore Casuale')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasso di Falsi Positivi (FPR)', fontsize=12)\n",
        "plt.ylabel('Tasso di Veri Positivi (TPR)', fontsize=12)\n",
        "plt.title('Curva ROC', fontsize=14)\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"grafici/roc_curve_final.png\")\n",
        "plt.close()\n",
        "\n",
        "# 6.4. Metriche di Performance Complessive (OVERALL)\n",
        "metrics_names_overall = ['Accuratezza', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "metrics_values_overall = [acc, prec_weighted, rec_weighted, f1_weighted, roc_auc_final]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(metrics_names_overall, metrics_values_overall, color=sns.color_palette(\"viridis\", len(metrics_names_overall)))\n",
        "\n",
        "# Aggiungi i valori sulle barre\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.005, f'{yval:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.ylabel('Valore Metrica', fontsize=12)\n",
        "plt.xlabel('Metrica', fontsize=12)\n",
        "plt.title('Metriche di Performance Complessive', fontsize=14)\n",
        "plt.ylim(0.0, 1.05)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"grafici/overall_performance_metrics.png\")\n",
        "plt.close()\n",
        "\n",
        "# 6.5. Distribuzione delle Probabilità Predette (Classe positiva)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(all_probs_positive_class, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.xlabel(\"Probabilità della classe '1' (Positiva)\", fontsize=12) # Specifica la classe\n",
        "plt.ylabel(\"Frequenza\", fontsize=12)\n",
        "plt.title(\"Distribuzione delle Probabilità Predette\", fontsize=14)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"grafici/prob_distribution.png\")\n",
        "plt.close()\n",
        "\n",
        "# 7. Salvataggio DataFrame dei risultati\n",
        "results_df = pd.DataFrame({\n",
        "    'predicted': all_preds,\n",
        "    'true': all_labels,\n",
        "    'prob_positive': all_probs_positive_class\n",
        "})\n",
        "results_df.to_csv('predizioni_test.csv', index=False)\n",
        "\n",
        "# 8. Salvataggio report di classificazione finale su file\n",
        "with open(\"risultati/final_metrics_report.txt\", \"w\") as f:\n",
        "    f.write(\"--- REPORT DI CLASSIFICAZIONE DETTAGLIATO ---\\n\\n\")\n",
        "    f.write(classification_report(all_labels, all_preds, digits=3)) # digits=3 per più precisione\n",
        "    f.write(f\"\\nAccuracy (globale): {acc:.3f}\")\n",
        "    f.write(f\"\\nROC-AUC (globale): {roc_auc:.3f}\")\n",
        "    f.write(f\"\\nPrecisione (Weighted): {prec_weighted:.3f}\")\n",
        "    f.write(f\"\\nRichiamo (Weighted): {rec_weighted:.3f}\")\n",
        "    f.write(f\"\\nF1-Score (Weighted): {f1_weighted:.3f}\")\n",
        "\n",
        "print(\"\\nReport di classificazione finale salvato in 'risultati/final_metrics_report.txt'\")\n",
        "print(\"Tutti i grafici di performance sono stati generati nella cartella 'grafici/'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ruosRKgLbcR",
        "outputId": "f594468d-4bff-499a-eb72-0352fd8070c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "REPORT DI CLASSIFICAZIONE:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.84      0.87      2448\n",
            "           1       0.86      0.90      0.88      2552\n",
            "\n",
            "    accuracy                           0.87      5000\n",
            "   macro avg       0.87      0.87      0.87      5000\n",
            "weighted avg       0.87      0.87      0.87      5000\n",
            "\n",
            "==============================\n",
            "\n",
            "Accuracy: 0.873\n",
            "ROC-AUC: 0.952\n",
            "\n",
            "Report di classificazione finale salvato in 'risultati/final_metrics_report.txt'\n",
            "Tutti i grafici di performance sono stati generati nella cartella 'grafici/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Dati della matrice di confusione calcolati\n",
        "# Queste sono le count, non le percentuali\n",
        "tn = 2022  # True Negatives (Reale 0, Predetto 0)\n",
        "fp = 426   # False Positives (Reale 0, Predetto 1)\n",
        "fn = 193   # False Negatives (Reale 1, Predetto 0)\n",
        "tp = 2359  # True Positives (Reale 1, Predetto 1)\n",
        "\n",
        "confusion_matrix = np.array([[tn, fp],\n",
        "                             [fn, tp]])\n",
        "# Nomi delle classi\n",
        "class_names = ['0', '1']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='.0f', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names, cbar=False, linewidths=.5)\n",
        "\n",
        "\n",
        "plt.xlabel('Predict label', fontsize=14)\n",
        "plt.ylabel('True label', fontsize=14)\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "\n",
        "\n",
        "# Aggiustamenti per visualizzare correttamente\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"./grafici/conf_matrix_test.png\")"
      ],
      "metadata": {
        "id": "xs7scZOgLKFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EAFOdUN8fRy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Dataset: già disponibili\n",
        "# - X_struct (scaled)\n",
        "# - df_tfidf (TF-IDF as DataFrame)\n",
        "# - y = df['label'].values\n",
        "\n",
        "# Combinazioni di feature\n",
        "X_tfidf\n",
        "y = df['label'].values\n",
        "\n",
        "# Definizione set\n",
        "feature_sets = {\n",
        "    'Struttural only': X_struct,\n",
        "    'Struttural + TF-IDF': X_tfidf\n",
        "}\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "generalization_test_size = 5000\n",
        "random_state = 42\n",
        "\n",
        "# Primo split: estraiamo 5k esempi per test di generalizzazione (random stratificato)\n",
        "X_struct_rest, X_struct_generalization, y_rest, y_generalization = train_test_split(\n",
        "    X_struct, y,\n",
        "    test_size=generalization_test_size,\n",
        "    random_state=random_state,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_tfidf_rest, X_tfidf_generalization, _, _ = train_test_split(\n",
        "    X_tfidf, y,\n",
        "    test_size=generalization_test_size,\n",
        "    random_state=random_state,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Ora split train/test interno sul resto (ad esempio 70/30)\n",
        "test_size_internal = 0.3\n",
        "\n",
        "X_train_dict = {}\n",
        "X_test_dict = {}\n",
        "\n",
        "for name, X_rest in {'Struttural only': X_struct_rest, 'Struttural + TF-IDF': X_tfidf_rest}.items():\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_rest, y_rest,\n",
        "        test_size=test_size_internal,\n",
        "        random_state=random_state,\n",
        "        stratify=y_rest\n",
        "    )\n",
        "    X_train_dict[name] = X_train\n",
        "    X_test_dict[name] = X_test\n",
        "\n",
        "# Ora hai:\n",
        "# - X_train_dict, X_test_dict, y_train, y_test per train/test interno\n",
        "# - X_struct_generalization, X_tfidf_generalization, y_generalization per test di generalizzazione finale\n",
        "\n",
        "\n",
        "# === MODELLI E PARAMETRI ===\n",
        "models = {\n",
        "    # 'Random Forest': {\n",
        "    #     'model': RandomForestClassifier(),\n",
        "    #     'params': {\n",
        "    #         'n_estimators': [100, 200],\n",
        "    #         'max_depth': [None, 10, 20]\n",
        "    #     }\n",
        "    # },\n",
        "    'SVM': {\n",
        "        'model': LinearSVC(dual=False, max_iter=5000),\n",
        "        'params': {\n",
        "            'C': [0.1, 0.5, 1]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# === CICLO DI VALUTAZIONE ===\n",
        "results = []\n",
        "\n",
        "for model_name, config in models.items():\n",
        "    for feature_name in feature_sets.keys():\n",
        "        print(f\"\\n {model_name} con {feature_name}\")\n",
        "        X_train = X_train_dict[feature_name]\n",
        "        X_test = X_test_dict[feature_name]\n",
        "\n",
        "        print(X_train.shape)\n",
        "\n",
        "        grid = GridSearchCV(\n",
        "            config['model'],\n",
        "            config['params'],\n",
        "            # scoring='f1_weighted',\n",
        "            scoring='accuracy',\n",
        "            cv=5,\n",
        "            n_jobs=1\n",
        "        )\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        best_model = grid.best_estimator_\n",
        "\n",
        "        # Valutazione training set\n",
        "        y_pred_train = best_model.predict(X_train)\n",
        "        acc_train = accuracy_score(y_train, y_pred_train)\n",
        "        report_train = classification_report(y_train, y_pred_train, output_dict=True, zero_division=0)\n",
        "        f1_train = report_train['weighted avg']['f1-score']\n",
        "\n",
        "        # Valutazione test set\n",
        "        y_pred_test = best_model.predict(X_test)\n",
        "\n",
        "        # cm = confusion_matrix(y_test, y_pred_test)\n",
        "        # # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
        "\n",
        "        # plt.figure(figsize=(5, 4))\n",
        "        # cm.plot(cmap=\"Blues\", values_format='d')\n",
        "        # plt.title(f\"{model_name} | {feature_name}\")\n",
        "        # plt.grid(False)\n",
        "        # plt.tight_layout()\n",
        "        # plt.savefig(\"./grafici/confusion_matrix_\" + model_name + \"_\" + feature_name)\n",
        "        # plt.close()\n",
        "\n",
        "\n",
        "        acc_test = accuracy_score(y_test, y_pred_test)\n",
        "        prec = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
        "        report_test = classification_report(y_test, y_pred_test, output_dict=True, zero_division=0)\n",
        "        f1_test = report_test['weighted avg']['f1-score']\n",
        "        # Addestramento e valutazione con il modello ottimizzato\n",
        "        Evaluation(best_model, X_train, X_test, y_train, y_test).train_evaluation()\n",
        "        Evaluation(best_model, X_train, X_test, y_train, y_test).test_evaluation()\n",
        "\n",
        "        # Cross-validation score (5-fold sul training set)\n",
        "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "        cv_mean = np.mean(cv_scores)\n",
        "        cv_std = np.std(cv_scores)\n",
        "\n",
        "        # Salvataggio modello e parametri\n",
        "        model_filename = f\"modelli/{model_name}_{feature_name}_best_model.pkl\"\n",
        "        joblib.dump(best_model, model_filename)\n",
        "\n",
        "        params_filename = f\"modelli/{model_name}_{feature_name}_params.json\"\n",
        "        with open(params_filename, \"w\") as f:\n",
        "            import json\n",
        "            json.dump(grid.best_params_, f)\n",
        "\n",
        "            # ROC AUC - train\n",
        "            if hasattr(best_model, \"predict_proba\"):\n",
        "                y_train_score = best_model.predict_proba(X_train)[:, 1]\n",
        "                y_test_score = best_model.predict_proba(X_test)[:, 1]\n",
        "            elif hasattr(best_model, \"decision_function\"):\n",
        "                y_train_score = best_model.decision_function(X_train)\n",
        "                y_test_score = best_model.decision_function(X_test)\n",
        "            else:\n",
        "                y_train_score = None\n",
        "                y_test_score = None\n",
        "\n",
        "            train_auc = roc_auc_score(y_train, y_train_score) if y_train_score is not None else None\n",
        "            test_auc = roc_auc_score(y_test, y_test_score) if y_test_score is not None else None\n",
        "\n",
        "        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
        "        metrics_values = [acc_test, prec, rec, f1, test_auc]\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.barplot(x=metrics_names, y=metrics_values, palette=\"viridis\")\n",
        "        plt.ylim(0, 1)\n",
        "        plt.title(f\"Performance Metrics - {model_name} with {feature_name}\")\n",
        "        for i, v in enumerate(metrics_values):\n",
        "            plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"./grafici/metrics_summary_{model_name}_{feature_name}.png\")\n",
        "        plt.close()\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Features': feature_name,\n",
        "            'Best Params': grid.best_params_,\n",
        "            'Train Accuracy': acc_train,\n",
        "            'Train F1': f1_train,\n",
        "            'Test Accuracy': acc_test,\n",
        "            'Test F1': f1_test,\n",
        "            'CV Accuracy Mean': cv_mean,\n",
        "            'CV Accuracy Std': cv_std,\n",
        "            'CV Scores': cv_scores,\n",
        "            'Train AUC': train_auc,\n",
        "            'Test AUC': test_auc\n",
        "        })\n",
        "\n",
        "        print(f\"Best Params: {grid.best_params_}\")\n",
        "        # print(f\"Train Accuracy: {acc_train:.4f} - Train F1: {f1_train:.4f}\")\n",
        "        # print(f\"Test Accuracy: {acc_test:.4f} - Test F1: {f1_test:.4f}\")\n",
        "        print(f\"CV F1 Mean: {cv_mean:.4f} - CV F1 Std: {cv_std:.4f}\")\n",
        "        # print(\"Classification Report Test Set:\\n\", classification_report(y_test, y_pred_test, zero_division=0))\n",
        "\n",
        "        # ROC Curve per il test set)\n",
        "        if y_test_score is not None:\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_test_score)\n",
        "            plt.figure()\n",
        "            plt.plot(fpr, tpr, label=f\"{model_name} ({feature_name}) AUC = {test_auc:.2f}\")\n",
        "            plt.plot([0, 1], [0, 1], 'k--')\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate\")\n",
        "            plt.title(f\"ROC Curve - {model_name} ({feature_name})\")\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"./grafici/ROC Curve - {model_name} ({feature_name}).png\")\n",
        "            plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzVGeBr48qj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "31491d4f-e8e6-4911-aae0-3ab5a706022d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Ecco il riepilogo dei risultati:\n",
            " Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Test Accuracy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-871526914.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m sns.barplot(\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7187\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7189\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7191\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Test Accuracy'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# === VISUALIZZAZIONE ===\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Esporto in CSV\n",
        "df_results.to_csv(\"./risultati/risultati_model_selection.csv\", index=False)\n",
        "\n",
        "print(\"\\n📊 Ecco il riepilogo dei risultati:\\n\", df_results)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    data=df_results.sort_values(by=\"Test Accuracy\", ascending=False),\n",
        "    x=\"Features\",\n",
        "    y=\"Test Accuracy\",\n",
        "    hue=\"Model\",\n",
        "    palette=\"Set2\"\n",
        ")\n",
        "plt.title(\"Confronto Accuracy tra Modelli e Feature (ordinato)\")\n",
        "plt.ylim(0.5, 1.0)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Tipo di Feature\")\n",
        "plt.legend(title=\"Modello\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./grafici/confronto_accuracy.png\")\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    data=df_results.sort_values(by=\"Test F1\", ascending=False),\n",
        "    x=\"Features\",\n",
        "    y=\"Test F1\",\n",
        "    hue=\"Model\",\n",
        "    palette=\"Set1\"\n",
        ")\n",
        "plt.title(\"Confronto F1-score tra Modelli e Feature (ordinato)\")\n",
        "plt.ylim(0.5, 1.0)\n",
        "plt.ylabel(\"F1-score\")\n",
        "plt.xlabel(\"Tipo di Feature\")\n",
        "plt.legend(title=\"Modello\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./grafici/confronto_f1_score.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "# --- Grafico a barre con media e deviazione standard (accuracy) ---\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "labels = df_results['Model'] + \" con \" + df_results['Features']\n",
        "means = df_results['CV Accuracy Mean']\n",
        "stds = df_results['CV Accuracy Std']\n",
        "\n",
        "plt.bar(labels, means, yerr=stds, capsize=5, color='skyblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Accuracy media (cross-validation)\")\n",
        "plt.title(\"Confronto modelli: media Accuracy cross-validation con deviazione standard\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Salvo il grafico in file PNG\n",
        "plt.savefig(\"./grafici/grafico_accuracy_cv.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n Grafico salvato come 'grafico_accuracy_cv.png'\")\n",
        "\n",
        "# BOXPLOT singoli fold score\n",
        "# HEATMAP F1 SCORE\n",
        "heatmap_df = df_results.pivot(index='Model', columns='Features', values='Test Accuracy')\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(heatmap_df, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n",
        "plt.title(\"F1 Score sul Test Set (modelli ottimizzati)\")\n",
        "plt.ylabel(\"Modello\")\n",
        "plt.xlabel(\"Feature Set\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./grafici/grafico_Accuracy_test_set.png\")\n",
        "plt.close()\n",
        "\n",
        "\n",
        "df_results['Model+Feature'] = df_results['Model'] + \" + \" + df_results['Features']\n",
        "# ✅ Salva su CSV\n",
        "df_results.to_csv(\"./risultati/risultati_finali_pipeline.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "generalization_test_size = 5000\n",
        "random_state = 42\n",
        "\n",
        "# Combinazioni di feature\n",
        "X_tfidf\n",
        "y = df['label'].values\n",
        "\n",
        "# Primo split: estraiamo 5k esempi per test di generalizzazione (random stratificato)\n",
        "X_struct_rest, X_struct_generalization, y_rest, y_generalization = train_test_split(\n",
        "    X_struct, y,\n",
        "    test_size=generalization_test_size,\n",
        "    random_state=random_state,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_tfidf_rest, X_tfidf_generalization, _, _ = train_test_split(\n",
        "    X_tfidf, y,\n",
        "    test_size=generalization_test_size,\n",
        "    random_state=random_state,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "X_test_final = X_tfidf_generalization\n",
        "y_test = y_generalization\n",
        "\n",
        "\n",
        "# 🧠 Carica modelli\n",
        "# rf_model = joblib.load(\"./modelli/Random Forest_Struttural + TF-IDF_best_model.pkl\")\n",
        "svm_model = joblib.load(\"./modelli/SVM_Struttural + TF-IDF_best_model.pkl\")\n",
        "# rf_model2 = joblib.load(\"./modelli/Random Forest_Struttural only_best_model.pkl\")\n",
        "# svm_model2 = joblib.load(\"./modelli/SVM_Struttural only_best_model.pkl\")\n",
        "\n",
        "# ✅ Valutazione modelli classici\n",
        "def evaluate_model(model, X, y, name):\n",
        "    y_pred = model.predict(X)\n",
        "    y_prob = model.predict_proba(X)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
        "    print(\"F1-score:\", f1_score(y, y_pred, average=\"weighted\"))\n",
        "    print(\"Precision: \", precision_score(y, y_pred, average=\"weighted\"))\n",
        "    print(\"Recall\", recall_score(y, y_pred, average=\"weighted\"))\n",
        "    if y_prob is not None:\n",
        "        print(\"ROC AUC:\", roc_auc_score(y, y_prob))\n",
        "    print(\"Classification Report:\\n\", classification_report(y, y_pred))\n",
        "    return y_pred, y_prob\n",
        "\n",
        "# y_pred_rf, y_prob_rf = evaluate_model(rf_model, X_test_final, y_test, \"Random Forest\")\n",
        "y_pred_svm, y_prob_svm = evaluate_model(svm_model, X_test_final, y_test, \"SVM\")\n",
        "\n",
        "\n",
        "result_df = pd.DataFrame()\n",
        "result_df[\"Label esatta\"] = y_test\n",
        "result_df[\"Label predetta\"] = y_pred_svm\n",
        "\n",
        "# Mappatura dei valori numerici a stringhe\n",
        "label_map = {0: \"fake\", 1: \"real\"}\n",
        "result_df[\"Label esatta\"] = result_df[\"Label esatta\"].map(label_map)\n",
        "result_df[\"Label predetta\"] = result_df[\"Label predetta\"].map(label_map)\n",
        "\n",
        "print(result_df.head())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_curve, roc_auc_score, accuracy_score,\n",
        "    f1_score, classification_report, precision_score, recall_score\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "def plot_confusion(y_true, y_pred, name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    disp.plot(cmap=\"Blues\", values_format='d')\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.grid(False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"./grafici/conf_matrix_{name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc(y_true, y_score, name):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    auc = roc_auc_score(y_true, y_score)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"ROC Curve - {name}\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"./grafici/roc_curve_{name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics(y_true, y_pred, y_score, name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "    auc = roc_auc_score(y_true, y_score) if y_score is not None else 0\n",
        "    pre = precision_score(y_true, y_pred, average=\"weighted\")\n",
        "    rec = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "    metrics_names = ['Accuracy', 'F1 Score', 'ROC AUC', 'Recall', 'Precision']\n",
        "    values = [acc, f1, auc, rec, pre]\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    sns.barplot(x=metrics_names, y=values, palette=\"viridis\")\n",
        "    plt.ylim(0, 1)\n",
        "    for i, v in enumerate(values):\n",
        "        plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
        "    plt.title(f\"Metrics - {name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"./grafici/metrics_{name}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# === RF ===\n",
        "# plot_confusion(y_test, y_pred_rf, \"Random_Forest\")\n",
        "# if y_prob_rf is not None:\n",
        "#     plot_roc(y_test, y_prob_rf, \"Random_Forest\")\n",
        "# plot_metrics(y_test, y_pred_rf, y_prob_rf, \"Random_Forest\")\n",
        "\n",
        "# === SVM ===\n",
        "plot_confusion(y_test, y_pred_svm, \"SVM\")\n",
        "if y_prob_svm is not None:\n",
        "    plot_roc(y_test, y_prob_svm, \"SVM\")\n",
        "plot_metrics(y_test, y_pred_svm, y_prob_svm, \"SVM\")\n"
      ],
      "metadata": {
        "id": "ZMfjMFHFTF8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "add6cc8b-1fc1-4668-8398-30f3b44a9ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SVM =====\n",
            "Accuracy: 0.9396\n",
            "F1-score: 0.9395842532112936\n",
            "Precision:  0.9397142276995968\n",
            "Recall 0.9396\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94      2448\n",
            "           1       0.93      0.95      0.94      2552\n",
            "\n",
            "    accuracy                           0.94      5000\n",
            "   macro avg       0.94      0.94      0.94      5000\n",
            "weighted avg       0.94      0.94      0.94      5000\n",
            "\n",
            "  Label esatta Label predetta\n",
            "0         fake           fake\n",
            "1         fake           fake\n",
            "2         fake           real\n",
            "3         real           real\n",
            "4         real           real\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-934091633.py:115: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=metrics_names, y=values, palette=\"viridis\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}